{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import sys\n",
    "\n",
    "# Read the original JSON file\n",
    "with open('pre-parsed_messages.json', 'r', encoding='utf-8') as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "# Define the list of keywords\n",
    "keywords = [\"–ø—É—Ç–∏–Ω\", \"—Å–≤–æ\", \"–ø—Ä–∏–≥–æ–∂–∏–Ω\", \"–≤–∞–≥–Ω–µ—Ä\", \"—É–∫—Ä–∞–∏–Ω–∞\"]\n",
    "\n",
    "# Define the mapping of periods to week numbers\n",
    "periods_map = {\n",
    "    \"27.05.2023-03.06.2023\": 1,\n",
    "    \"04.06.2023-10.06.2023\": 2,\n",
    "    \"11.06.2023-17.06.2023\": 3,\n",
    "    \"18.06.2023-24.06.2023\": 4,\n",
    "    \"25.06.2023-01.07.2023\": 5,\n",
    "    \"02.07.2023-08.07.2023\": 6,\n",
    "    \"09.07.2023-15.07.2023\": 7,\n",
    "    \"16.07.2023-22.07.2023\": 8\n",
    "}\n",
    "\n",
    "# Define a dictionary to store aggregated data\n",
    "aggregated_data = {}\n",
    "chanel_city_region_map = {}\n",
    "chanel_proximity = {}\n",
    "\n",
    "# Initialize the aggregated data dictionary\n",
    "for channel_url in set(item['channel_url'] for item in original_data):\n",
    "    aggregated_data[channel_url] = {week: {'message_count': 0, 'emoticon_count': {}, 'sentiment_count': {'positive': {'count': 0}, 'negative': {'count': 0}, 'neutral': {'count': 0}}, 'keyword_count': {keyword: {'count': 0, 'emoticon_count': {}, 'sentiment_count': {'positive': {'count': 0}, 'negative': {'count': 0}, 'neutral': {'count': 0}}} for keyword in keywords}} for week in range(1, 9)}\n",
    "    \n",
    "    \n",
    "\n",
    "# Iterate through each message in the original JSON\n",
    "for item in original_data:\n",
    "    channel_url = item['channel_url']\n",
    "    chanel_city_region_map[channel_url] = {'city': item['city'], 'region': item['region']}\n",
    "    message_timestamp = datetime.strptime(item['date'], '%Y-%m-%dT%H:%M:%S%z')\n",
    "    proximity = 1 if \"–†–æ—Å—Ç–æ–≤\" in item['city'] or \"–í–æ—Ä–æ–Ω–µ–∂\" in item['city'] or \"–ï–ª–µ—Ü\" in item['city'] else (2 if \"–†–æ—Å—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å\" in item['region'] or \"–í–æ—Ä–æ–Ω–µ–∂—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å\" in item['region'] or \"–õ–∏–ø–µ—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å\" in item['region'] else 3)  \n",
    "    chanel_proximity[channel_url] = {'proximity': proximity}\n",
    "\n",
    "    if item.get('message') is None:\n",
    "        message_content = ''\n",
    "    else:\n",
    "        message_content = item.get('message')\n",
    "\n",
    "    if item.get('reactions') is None:\n",
    "        reactions = {}\n",
    "    else:\n",
    "        reactions = item.get('reactions', {}).get('results', [])\n",
    "\n",
    "    week_number = None\n",
    "    \n",
    "    for period, num in periods_map.items():\n",
    "        start_date, end_date = period.split('-')\n",
    "        start_date = datetime.strptime(start_date, '%d.%m.%Y')\n",
    "        end_date = datetime.strptime(end_date, '%d.%m.%Y') + timedelta(days=1)\n",
    "        start_date = start_date.replace(tzinfo=timezone.utc)\n",
    "        end_date = end_date.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        if start_date <= message_timestamp < end_date:\n",
    "            week_number = num\n",
    "            break\n",
    "    \n",
    "    if week_number:\n",
    "        aggregated_data[channel_url][week_number]['message_count'] += 1\n",
    "        for reaction in reactions:\n",
    "            emoticon = reaction.get('reaction', {}).get('emoticon')\n",
    "            count = reaction.get('count', 0)\n",
    "           \n",
    "            sentiment = \"positive\" if \"üòé\" in emoticon or \"üòç\" in emoticon or \"üëå\" in emoticon or \"üî•\" in emoticon or \"ü§©\" in emoticon or \"ü•∞\" in emoticon or \"üíò\" in emoticon or \"üíØ\" in emoticon or \"üéâ\" in emoticon or \"‚ù§‚Äçüî•\" in emoticon or \"ü§ó\" in emoticon or \"üëç\" in emoticon or \"üòÅ\" in emoticon or \"ü§£\" in emoticon or \"üÜí\" in emoticon or \"üèÜ\" in emoticon or \"üçæ\" in emoticon or \"‚ù§\" in emoticon else (\"negative\" if \"üò°\" in emoticon or \"ü•¥\" in emoticon or \"üò≠\" in emoticon or \"üëé\" in emoticon or \"ü§¨\" in emoticon or \"üíî\" in emoticon or \"üò¢\" in emoticon or \"üñï\" in emoticon or \"ü§°\" in emoticon or \"üò®\" in emoticon or \"üí©\" in emoticon or \"ü§Æ\" in emoticon or \"üò±\" in emoticon else \"neutral\")\n",
    "            aggregated_data[channel_url][week_number]['sentiment_count'][sentiment]['count'] += 1\n",
    "\n",
    "            if emoticon:\n",
    "                if emoticon in aggregated_data[channel_url][week_number]['emoticon_count']:\n",
    "                    aggregated_data[channel_url][week_number]['emoticon_count'][emoticon] += count\n",
    "                else:\n",
    "                    aggregated_data[channel_url][week_number]['emoticon_count'][emoticon] = count\n",
    "        \n",
    "        # Count keyword occurrences in the message content\n",
    "        for keyword in keywords:\n",
    "            if keyword in message_content.lower():\n",
    "                aggregated_data[channel_url][week_number]['keyword_count'][keyword]['count'] += 1\n",
    "                for reaction in reactions:\n",
    "                    emoticon = reaction.get('reaction', {}).get('emoticon')\n",
    "                    count = reaction.get('count', 0)\n",
    "                    \n",
    "                    sentiment = \"positive\" if \"üòé\" in emoticon or \"üòç\" in emoticon or \"üëå\" in emoticon or \"üî•\" in emoticon or \"ü§©\" in emoticon or \"ü•∞\" in emoticon or \"üíò\" in emoticon or \"üíØ\" in emoticon or \"üéâ\" in emoticon or \"‚ù§‚Äçüî•\" in emoticon or \"ü§ó\" in emoticon or \"üëç\" in emoticon or \"üòÅ\" in emoticon or \"ü§£\" in emoticon or \"üÜí\" in emoticon or \"üèÜ\" in emoticon or \"üçæ\" in emoticon or \"‚ù§\" in emoticon else (\"negative\" if \"üò°\" in emoticon or \"ü•¥\" in emoticon or \"üò≠\" in emoticon or \"üëé\" in emoticon or \"ü§¨\" in emoticon or \"üíî\" in emoticon or \"üò¢\" in emoticon or \"üñï\" in emoticon or \"ü§°\" in emoticon or \"üò®\" in emoticon or \"üí©\" in emoticon or \"ü§Æ\" in emoticon or \"üò±\" in emoticon else \"neutral\")\n",
    "                    aggregated_data[channel_url][week_number]['keyword_count'][keyword]['sentiment_count'][sentiment]['count'] += 1\n",
    "\n",
    "                    if emoticon:\n",
    "                        if emoticon in aggregated_data[channel_url][week_number]['keyword_count'][keyword]['emoticon_count']:\n",
    "                            aggregated_data[channel_url][week_number]['keyword_count'][keyword]['emoticon_count'][emoticon] += count\n",
    "                        else:\n",
    "                            aggregated_data[channel_url][week_number]['keyword_count'][keyword]['emoticon_count'][emoticon] = count\n",
    "# Convert the aggregated data to a list of dictionaries\n",
    "result = [{'channel_url': channel_url, 'weeks': weeks, 'city': chanel_city_region_map[channel_url]['city'], 'region': chanel_city_region_map[channel_url]['region'], 'proximity': chanel_proximity[channel_url]['proximity']} for channel_url, weeks in aggregated_data.items()]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
